{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 12.03 - Casos Reales de Scraping\n",
    "\n",
    "**Autor:** Miguel Angel Vazquez Varela  \n",
    "**Nivel:** Intermedio  \n",
    "**Tiempo estimado:** 35 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ¿Qué aprenderás?\n",
    "\n",
    "- Scrapear múltiples páginas (paginación)\n",
    "- Manejar estructuras HTML irregulares\n",
    "- Limpiar datos extraídos con pandas\n",
    "- Guardar los resultados en CSV/JSON\n",
    "- Implementar retries y delays responsables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Paginación\n",
    "\n",
    "La mayoría de sitios dividen los resultados en páginas. Hay que detectar el patrón de la URL y iterar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def scrape_page(url: str, headers: dict) -> BeautifulSoup | None:\n",
    "    \"\"\"Descarga una página y retorna el soup. Maneja errores.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.text, 'lxml')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error en {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_paginated(base_url: str, max_pages: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scraping con paginación genérica.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_url : str\n",
    "        URL base con placeholder {page} para el número de página\n",
    "    max_pages : int\n",
    "        Número máximo de páginas a scrapear\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict]\n",
    "        Lista de registros extraídos\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "    all_records = []\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = base_url.format(page=page)\n",
    "        print(f\"Scrapeando página {page}: {url}\")\n",
    "\n",
    "        soup = scrape_page(url, headers)\n",
    "        if soup is None:\n",
    "            break\n",
    "\n",
    "        # Extraer registros de esta página (ajustar selectores al sitio real)\n",
    "        rows = soup.select('table.data tr')\n",
    "        if not rows:\n",
    "            print(f\"Sin datos en página {page}, deteniendo\")\n",
    "            break\n",
    "\n",
    "        for row in rows[1:]:  # Saltar la cabecera\n",
    "            cells = row.find_all('td')\n",
    "            if cells:\n",
    "                all_records.append({\n",
    "                    'col1': cells[0].text.strip(),\n",
    "                    'col2': cells[1].text.strip() if len(cells) > 1 else None,\n",
    "                })\n",
    "\n",
    "        time.sleep(1)  # Respetar el servidor: 1 segundo entre peticiones\n",
    "\n",
    "    return all_records\n",
    "\n",
    "\n",
    "print(\"Función de paginación definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Caso real: datos de la API de OpenData Madrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API pública del Ayuntamiento de Madrid - datos de BiciMAD\n",
    "# No requiere autenticación para datos básicos\n",
    "\n",
    "def fetch_bicimad_data() -> pd.DataFrame | None:\n",
    "    \"\"\"Obtiene datos actuales de estaciones BiciMAD.\"\"\"\n",
    "    url = \"https://api.citybik.es/v2/networks/bicimad\"\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        stations = data['network']['stations']\n",
    "        df = pd.DataFrame(stations)\n",
    "\n",
    "        # Limpiar y seleccionar columnas relevantes\n",
    "        df = df[['name', 'free_bikes', 'empty_slots', 'latitude', 'longitude', 'timestamp']]\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df['total_docks'] = df['free_bikes'] + df['empty_slots']\n",
    "        df['occupancy_pct'] = (df['free_bikes'] / df['total_docks'] * 100).round(1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except (requests.exceptions.RequestException, KeyError) as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_bicimad = fetch_bicimad_data()\n",
    "\n",
    "if df_bicimad is not None:\n",
    "    print(f\"Estaciones: {len(df_bicimad)}\")\n",
    "    print(f\"\\nTop 5 estaciones con más bicis disponibles:\")\n",
    "    print(df_bicimad.nlargest(5, 'free_bikes')[['name', 'free_bikes', 'occupancy_pct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Limpieza de datos scrapeados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los datos scrapeados suelen venir sucios: espacios, caracteres raros, tipos incorrectos\n",
    "\n",
    "raw_data = [\n",
    "    {'name': '  Sol   ', 'bikes': '12 bicis', 'lat': '40.4168°N'},\n",
    "    {'name': 'Atocha\\n', 'bikes': '5',       'lat': '40.4089°N'},\n",
    "    {'name': 'Cibeles', 'bikes': 'N/A',      'lat': '40.4194°N'},\n",
    "    {'name': 'Retiro',  'bikes': '18 bicis', 'lat': '40.4153°N'},\n",
    "]\n",
    "\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "print(\"Datos crudos:\")\n",
    "print(df_raw)\n",
    "print()\n",
    "\n",
    "def clean_scraped_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Limpia un DataFrame con datos scrapeados típicos.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Limpiar strings\n",
    "    df['name'] = df['name'].str.strip()\n",
    "\n",
    "    # Extraer números de strings mixtos\n",
    "    df['bikes'] = pd.to_numeric(\n",
    "        df['bikes'].str.extract(r'(\\d+)')[0],\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Limpiar coordenadas\n",
    "    df['lat'] = df['lat'].str.replace('°N', '').astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_clean = clean_scraped_df(df_raw)\n",
    "print(\"Datos limpios:\")\n",
    "print(df_clean)\n",
    "print(f\"\\nNulos en bikes: {df_clean['bikes'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Guardar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def save_scraped_data(df: pd.DataFrame, name: str, output_dir: str = 'data/raw') -> None:\n",
    "    \"\"\"\n",
    "    Guarda datos scrapeados con timestamp en el nombre.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Datos a guardar\n",
    "    name : str\n",
    "        Nombre base del archivo\n",
    "    output_dir : str\n",
    "        Directorio de salida\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    csv_path = f\"{output_dir}/{name}_{timestamp}.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Guardado: {csv_path} ({len(df)} filas)\")\n",
    "\n",
    "\n",
    "# Guardar los datos de BiciMAD si los obtuvimos\n",
    "if df_bicimad is not None:\n",
    "    save_scraped_data(df_bicimad, 'bicimad_stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Retries con backoff exponencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def fetch_with_retry(url: str, max_retries: int = 3, backoff: float = 2.0) -> requests.Response | None:\n",
    "    \"\"\"\n",
    "    Realiza una petición GET con reintentos y backoff exponencial.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL a descargar\n",
    "    max_retries : int\n",
    "        Número máximo de reintentos\n",
    "    backoff : float\n",
    "        Factor de espera (se multiplica en cada intento)\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "    wait = 1.0\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Intento {attempt}/{max_retries} fallido: {e}\")\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Esperando {wait:.1f}s antes de reintentar...\")\n",
    "                time.sleep(wait)\n",
    "                wait *= backoff  # 1s → 2s → 4s\n",
    "\n",
    "    print(f\"Fallaron todos los intentos para: {url}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Probar con URL válida\n",
    "resp = fetch_with_retry(\"https://api.citybik.es/v2/networks/bicimad\")\n",
    "if resp:\n",
    "    print(f\"Éxito: {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Técnica | Cuándo usarla |\n",
    "|---|---|\n",
    "| Paginación con `{page}` | URL cambia con número de página |\n",
    "| `str.extract(r'(\\d+)')` | Extraer números de texto mixto |\n",
    "| `pd.to_numeric(..., errors='coerce')` | Convertir sin fallar en N/A |\n",
    "| Timestamp en nombre de archivo | Datos que cambian con el tiempo |\n",
    "| Backoff exponencial | Evitar sobrecargar el servidor |\n",
    "| `time.sleep(1)` | Respetar rate limits |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio\n",
    "\n",
    "Construye un scraper que cada 5 minutos obtenga los datos de BiciMAD, los limpie y los guarde en CSV con timestamp. Analiza cómo cambia la disponibilidad de bicis a lo largo del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu solución aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Anterior:** [12.02 - BeautifulSoup](./12_02_beautifulsoup.ipynb)  \n",
    "**Siguiente:** [12.04 - Selenium](./12_04_selenium.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 12.05 - Ética y Buenas Prácticas en Scraping\n",
    "\n",
    "**Autor:** Miguel Angel Vazquez Varela  \n",
    "**Nivel:** Intermedio  \n",
    "**Tiempo estimado:** 20 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ¿Qué aprenderás?\n",
    "\n",
    "- Leer e interpretar `robots.txt`\n",
    "- Respetar rate limits y no saturar servidores\n",
    "- Aspectos legales básicos del scraping\n",
    "- Alternativas al scraping cuando existen\n",
    "- Checklist antes de scrapear un sitio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. robots.txt: las reglas del sitio\n",
    "\n",
    "Todo sitio web puede incluir un archivo `robots.txt` en su raíz que indica qué rutas pueden visitar los bots. Respetarlo es una obligación ética y en muchos casos legal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def can_fetch(url: str, user_agent: str = '*') -> bool:\n",
    "    \"\"\"\n",
    "    Comprueba si robots.txt permite scrapear una URL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL que queremos scrapear\n",
    "    user_agent : str\n",
    "        Nuestro user agent\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True si está permitido, False si no\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    robots_url = f\"{parsed.scheme}://{parsed.netloc}/robots.txt\"\n",
    "\n",
    "    rp = RobotFileParser()\n",
    "    rp.set_url(robots_url)\n",
    "\n",
    "    try:\n",
    "        rp.read()\n",
    "        return rp.can_fetch(user_agent, url)\n",
    "    except Exception:\n",
    "        # Si no hay robots.txt, asumimos que está permitido\n",
    "        return True\n",
    "\n",
    "\n",
    "# Ejemplos\n",
    "test_urls = [\n",
    "    \"https://es.wikipedia.org/wiki/Madrid\",\n",
    "    \"https://api.citybik.es/v2/networks\",\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    allowed = can_fetch(url)\n",
    "    status = '✓ Permitido' if allowed else '✗ Bloqueado'\n",
    "    print(f\"{status}: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el contenido del robots.txt de Wikipedia\n",
    "response = requests.get(\n",
    "    \"https://es.wikipedia.org/robots.txt\",\n",
    "    headers={'User-Agent': 'DataPortfolio/1.0'}\n",
    ")\n",
    "\n",
    "# Mostrar las primeras líneas\n",
    "for line in response.text.split('\\n')[:30]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Rate limiting: no saturar el servidor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"\n",
    "    Limita la velocidad de peticiones a un servidor.\n",
    "    Añade variación aleatoria para parecer más humano.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_delay: float = 1.0, max_delay: float = 3.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        min_delay : float\n",
    "            Espera mínima en segundos entre peticiones\n",
    "        max_delay : float\n",
    "            Espera máxima en segundos entre peticiones\n",
    "        \"\"\"\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self._last_request = 0.0\n",
    "\n",
    "    def wait(self) -> None:\n",
    "        \"\"\"Espera el tiempo necesario antes de la siguiente petición.\"\"\"\n",
    "        elapsed = time.time() - self._last_request\n",
    "        delay = random.uniform(self.min_delay, self.max_delay)\n",
    "\n",
    "        if elapsed < delay:\n",
    "            time.sleep(delay - elapsed)\n",
    "\n",
    "        self._last_request = time.time()\n",
    "\n",
    "\n",
    "def scrape_urls(urls: list[str]) -> list[str]:\n",
    "    \"\"\"Scraping de múltiples URLs respetando rate limits.\"\"\"\n",
    "    limiter = RateLimiter(min_delay=1.0, max_delay=2.0)\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "    results = []\n",
    "\n",
    "    for i, url in enumerate(urls, 1):\n",
    "        limiter.wait()  # Esperar antes de cada petición\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            results.append(response.text)\n",
    "            print(f\"[{i}/{len(urls)}] OK: {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"[{i}/{len(urls)}] Error: {e}\")\n",
    "            results.append(None)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Ejemplo\n",
    "urls = [\n",
    "    \"https://api.citybik.es/v2/networks/bicimad\",\n",
    "    \"https://api.citybik.es/v2/networks/valenbisi\",\n",
    "]\n",
    "\n",
    "responses = scrape_urls(urls)\n",
    "print(f\"\\nRespuestas recibidas: {sum(1 for r in responses if r is not None)}/{len(urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Aspectos legales\n",
    "\n",
    "> **Aviso:** esto es orientación general, no asesoría legal. Consulta a un abogado para casos específicos.\n",
    "\n",
    "| Situación | Consideración |\n",
    "|---|---|\n",
    "| Datos públicos sin login | Generalmente permitido |\n",
    "| Datos detrás de login | Puede violar los ToS |\n",
    "| Datos personales (GDPR) | Requiere base legal |\n",
    "| ToS dice \"no scraping\" | Alto riesgo legal |\n",
    "| Uso comercial de los datos | Verifica licencia |\n",
    "| Reproducir datos con copyright | Prohibido |\n",
    "\n",
    "### Alternativas al scraping\n",
    "\n",
    "Antes de scrapear, comprueba si existe:\n",
    "1. **API oficial** del sitio\n",
    "2. **Dataset descargable** (CSV, JSON, XML)\n",
    "3. **Portal de Open Data** (datos.gob.es, datos.madrid.es)\n",
    "4. **RSS feed** para noticias y blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Checklist antes de scrapear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping_checklist(url: str) -> dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Ejecuta comprobaciones básicas antes de scrapear un sitio.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL objetivo\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, bool]\n",
    "        Resultado de cada comprobación\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    base = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "    results = {}\n",
    "\n",
    "    # 1. ¿Permite robots.txt?\n",
    "    results['robots_txt_permite'] = can_fetch(url)\n",
    "\n",
    "    # 2. ¿Tiene API pública?\n",
    "    api_indicators = ['/api/', '/v1/', '/v2/', 'api.']\n",
    "    results['posible_api_disponible'] = any(ind in url for ind in api_indicators)\n",
    "\n",
    "    # 3. ¿El sitio responde?\n",
    "    try:\n",
    "        resp = requests.get(url, headers=headers, timeout=5)\n",
    "        results['sitio_accesible'] = resp.status_code == 200\n",
    "    except Exception:\n",
    "        results['sitio_accesible'] = False\n",
    "\n",
    "    # 4. ¿Tiene sitemap?\n",
    "    try:\n",
    "        sitemap = requests.get(f\"{base}/sitemap.xml\", headers=headers, timeout=5)\n",
    "        results['tiene_sitemap'] = sitemap.status_code == 200\n",
    "    except Exception:\n",
    "        results['tiene_sitemap'] = False\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "url_test = \"https://api.citybik.es/v2/networks/bicimad\"\n",
    "checks = scraping_checklist(url_test)\n",
    "\n",
    "print(f\"Checklist para: {url_test}\\n\")\n",
    "for check, result in checks.items():\n",
    "    icon = '✓' if result else '✗'\n",
    "    print(f\"  {icon} {check.replace('_', ' ').capitalize()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resumen: reglas de oro del scraping ético\n",
    "\n",
    "| Regla | Por qué |\n",
    "|---|---|\n",
    "| Lee siempre `robots.txt` | Es la política del sitio |\n",
    "| Usa un `User-Agent` descriptivo | Transparencia con el servidor |\n",
    "| Añade delays entre peticiones | No saturar el servidor |\n",
    "| Usa variación aleatoria en delays | No parecer un bot agresivo |\n",
    "| Revisa los ToS | Evitar problemas legales |\n",
    "| Prefiere APIs a HTML | Más estable y legal |\n",
    "| No scrapes datos personales | GDPR y privacidad |\n",
    "| Cachea los datos | No repetir peticiones innecesarias |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Anterior:** [12.04 - Selenium](./12_04_selenium.ipynb)  \n",
    "**Inicio del módulo:** [12.01 - Requests Basics](./12_01_requests_basics.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

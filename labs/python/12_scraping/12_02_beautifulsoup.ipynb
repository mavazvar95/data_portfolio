{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 12.02 - BeautifulSoup: Parsear HTML\n",
    "\n",
    "**Autor:** Miguel Angel Vazquez Varela  \n",
    "**Nivel:** Intermedio  \n",
    "**Tiempo estimado:** 30 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ¿Qué aprenderás?\n",
    "\n",
    "- Entender la estructura del DOM HTML\n",
    "- Parsear HTML con BeautifulSoup\n",
    "- Seleccionar elementos con CSS selectors y métodos `.find()`\n",
    "- Extraer texto, atributos y tablas\n",
    "- Combinar `requests` + `BeautifulSoup` en un flujo completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. ¿Qué es el DOM?\n",
    "\n",
    "El HTML de una página es un árbol de nodos (`tags`). BeautifulSoup nos permite navegarlo como si fuera Python.\n",
    "\n",
    "```html\n",
    "<div class=\"station\">\n",
    "  <h2>Sol</h2>\n",
    "  <span class=\"bikes\">12 bicis</span>\n",
    "</div>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install beautifulsoup4 lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# HTML de ejemplo (simulando una página de estaciones)\n",
    "html = \"\"\"\n",
    "<html>\n",
    "<body>\n",
    "  <h1>Estaciones BiciMAD</h1>\n",
    "  <div class=\"station\" id=\"s1\">\n",
    "    <h2 class=\"name\">Sol</h2>\n",
    "    <span class=\"bikes\">12</span>\n",
    "    <span class=\"slots\">8</span>\n",
    "  </div>\n",
    "  <div class=\"station\" id=\"s2\">\n",
    "    <h2 class=\"name\">Atocha</h2>\n",
    "    <span class=\"bikes\">5</span>\n",
    "    <span class=\"slots\">15</span>\n",
    "  </div>\n",
    "  <div class=\"station\" id=\"s3\">\n",
    "    <h2 class=\"name\">Cibeles</h2>\n",
    "    <span class=\"bikes\">0</span>\n",
    "    <span class=\"slots\">20</span>\n",
    "  </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "print(type(soup))\n",
    "print(soup.title)  # None porque no hay <title>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Navegación básica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find() → primer elemento que coincide\n",
    "titulo = soup.find('h1')\n",
    "print(f\"Título: {titulo.text}\")\n",
    "\n",
    "# find_all() → lista de todos los que coinciden\n",
    "stations = soup.find_all('div', class_='station')\n",
    "print(f\"Estaciones encontradas: {len(stations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a atributos\n",
    "for station in stations:\n",
    "    station_id = station['id']\n",
    "    name = station.find('h2', class_='name').text\n",
    "    bikes = station.find('span', class_='bikes').text\n",
    "    slots = station.find('span', class_='slots').text\n",
    "    print(f\"{station_id}: {name} | Bicis: {bikes} | Huecos: {slots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. CSS Selectors con select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select() usa sintaxis CSS → más potente para estructuras complejas\n",
    "\n",
    "# Todos los h2 dentro de .station\n",
    "names = soup.select('div.station h2.name')\n",
    "print(\"Nombres:\", [n.text for n in names])\n",
    "\n",
    "# Elemento con ID específico\n",
    "sol = soup.select_one('#s1')\n",
    "print(f\"Estación Sol: {sol.find('h2').text}\")\n",
    "\n",
    "# Solo estaciones con bicis disponibles (no es posible con CSS puro,\n",
    "# lo hacemos en Python después de extraer)\n",
    "available = [\n",
    "    s for s in soup.select('div.station')\n",
    "    if int(s.find('span', class_='bikes').text) > 0\n",
    "]\n",
    "print(f\"\\nEstaciones con bicis: {len(available)}\")\n",
    "for s in available:\n",
    "    print(f\"  {s.find('h2').text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Extraer tablas HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "html_tabla = \"\"\"\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr><th>Estación</th><th>Bicis</th><th>Huecos</th><th>Distrito</th></tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td>Sol</td><td>12</td><td>8</td><td>Centro</td></tr>\n",
    "    <tr><td>Atocha</td><td>5</td><td>15</td><td>Arganzuela</td></tr>\n",
    "    <tr><td>Cibeles</td><td>0</td><td>20</td><td>Centro</td></tr>\n",
    "    <tr><td>Retiro</td><td>18</td><td>2</td><td>Retiro</td></tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\"\"\"\n",
    "\n",
    "# pandas.read_html() extrae tablas directamente\n",
    "df = pd.read_html(html_tabla)[0]\n",
    "print(df)\n",
    "print(f\"\\nEstaciones con bicis disponibles:\")\n",
    "print(df[df['Bicis'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Flujo completo: requests + BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia_table(url: str, table_index: int = 0) -> pd.DataFrame | None:\n",
    "    \"\"\"\n",
    "    Extrae una tabla de Wikipedia y la devuelve como DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : str\n",
    "        URL de la página de Wikipedia\n",
    "    table_index : int\n",
    "        Índice de la tabla a extraer (0 = primera)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame | None\n",
    "        Tabla extraída o None si hay error\n",
    "    \"\"\"\n",
    "    headers = {'User-Agent': 'DataPortfolio/1.0 (educational)'}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        tables = pd.read_html(response.text)\n",
    "        if table_index >= len(tables):\n",
    "            print(f\"Solo hay {len(tables)} tablas en la página\")\n",
    "            return None\n",
    "\n",
    "        return tables[table_index]\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error de red: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Ejemplo: sistemas de bicicletas compartidas en Wikipedia\n",
    "url = \"https://es.wikipedia.org/wiki/Bicicleta_compartida\"\n",
    "df = scrape_wikipedia_table(url, table_index=0)\n",
    "\n",
    "if df is not None:\n",
    "    print(f\"Filas: {len(df)} | Columnas: {list(df.columns)}\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resumen\n",
    "\n",
    "| Método | Uso |\n",
    "|---|---|\n",
    "| `BeautifulSoup(html, 'lxml')` | Parsear HTML |\n",
    "| `soup.find('tag', class_='x')` | Primer elemento |\n",
    "| `soup.find_all('tag')` | Todos los elementos |\n",
    "| `soup.select('div.clase h2')` | CSS selector |\n",
    "| `element.text` | Texto del nodo |\n",
    "| `element['atributo']` | Atributo HTML |\n",
    "| `pd.read_html(html)` | Tablas directamente |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio\n",
    "\n",
    "Extrae la tabla de los sistemas de bikesharing más grandes del mundo de Wikipedia y crea un DataFrame con las columnas: `Ciudad`, `País`, `Bicis`. Ordénalo por número de bicis de mayor a menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu solución aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Anterior:** [12.01 - Requests Basics](./12_01_requests_basics.ipynb)  \n",
    "**Siguiente:** [12.03 - Scraping Real Cases](./12_03_scraping_real_cases.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
